{"cells":[{"cell_type":"markdown","source":["####Scaling - Normalization & Standardization"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1c9b01ee-6bb1-4576-b63a-9edbdd822be9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# imports\n# hides all warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# import pandas for spark\nimport pyspark.pandas as ps\n# pandas \nimport pandas as pd\n# numpy\nimport numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"be44ec84-6afb-455c-9091-1a385a33ec88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# handle nulls\n# data frame handle nulls replace with ReplVals of the columns as per replBy vars\n\"\"\"\ndesc:\n    HandleNulls - handle nulls from all cols in df except exclCols \n                nulls being replaced replBy value\nusage: \n    HandleNulls(df, replBy, colClass) \nparams:\n    df datarame, \n    replBy - mean, median, minimum (of mean & median), maximum (of mean & median) \n    exclCols - col to ignore while transformation \n\"\"\"\ndef HandleNulls(df, replBy, lExclCols=[]):\n    # orig col names\n    colNames = df.columns.to_list()\n    # if not list convert to list\n    if not isinstance(lExclCols, list):\n        lExclCols = [lExclCols]\n    #print(lExclCols)\n    # if not empty, create a dataframe of ExclCols\n    if lExclCols != []:\n        for vExclCol in lExclCols:\n            colNames.remove(vExclCol)\n    # handle outlier for each col\n    for colName in colNames:\n        if ((df[colName].isnull()).sum() > 0):\n            if (replBy == \"mean\"):\n                replVals = df[colName].mean()\n            elif (replBy == \"median\"):\n                replVals = df[colName].median()\n            elif (replBy == \"minimum\"):\n                replVals = min(df[colName].mean(),df[colName].median())\n            elif (replBy == \"maximum\"):\n                replVals = max(df[colName].mean(),df[colName].median())\n            # replace\n            df[colName] = df[colName].fillna(replVals)\n    return df\n\n# data frame handle nulls replace with mean of the columns \ndef HandleNullsWithMean(df, lExclCols=[]):\n    df = HandleNulls(df, \"mean\", lExclCols)\n    return df\n\n# data frame handle nulls replace with median of the columns \ndef HandleNullsWithMedian(df, lExclCols=[]):\n    df = HandleNulls(df, \"median\", lExclCols)\n    return df\n\n# data frame handle nulls replace with min(mean,median) of the columns \ndef HandleNullsWithMinOfMM(df, lExclCols=[]):\n    df = HandleNulls(df, \"minimum\", lExclCols)\n    return df\n\n# data frame handle nulls replace with max(mean,median) of the columns \ndef HandleNullsWithMaxOfMM(df, lExclCols=[]):\n    df = HandleNulls(df, \"maximum\", lExclCols)\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"50d8b6b0-50bc-4d03-b88c-b5af2aed14cf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# normalization - single col\n# x_scaled = (x-min(x)) / (max(x)–min(x))\ndef colNormalization(colName,colValues):\n    #print(colName)\n    min = colValues.min()\n    max = colValues.max()\n    colValues = (colValues-min)/(max-min)\n    return colValues\n\n# standardization - single col\n# x_scaled = (x — mean(x)) / stddev(x)\ndef colStandardization(colName,colValues):\n    #print(colName)\n    mean = colValues.mean()\n    std = colValues.std()\n    colValues = (colValues-mean)/(std)\n    return colValues\n\n# scale data\n\"\"\"\ndesc:\n    normalize data - all cols of df will be Normalized except lExclCols\n    x_scaled = (x-min(x)) / (max(x)–min(x))\n    all values will be between 0 & 1 or if -ve values are present then -1 & 1\nusage: \n    ScaleData(df, 'N', lExclCols) \nparams:\n    df datarame, type, lExclCols - cols to ignore while transformation  \n\ndesc:\n    standardise data - all cols of df will be Standarized except lExclCols\n    x_scaled = (x — mean(x)) / stddev(x)\n    transforms data to have a mean of zero and a standard deviation of 1\n    ScaleData(df, 'S', lExclCols) \nparams:\n    df datarame, type, lExclCols - cols to ignore while transformation  \n\"\"\"\ndef ScaleData(df, type='N', lExclCols=[]):\n    # chech type\n    if (type != 'N') & (type != 'S'):\n        type = 'N'\n    # orig col names\n    colNames = df.columns.to_list()\n    # if not list convert to list\n    if not isinstance(lExclCols, list):\n        lExclCols = [lExclCols]\n    #print(lExclCols)\n    # if not empty, create a dataframe of ExclCols\n    if lExclCols != []:\n        for vExclCol in lExclCols:\n            colNames.remove(vExclCol)\n    # handle outlier for each col\n    for colName in colNames:\n        if (df[colName].dtypes == 'object'):\n            continue\n        if (type == 'N'):\n            colValues = colNormalization(colName,df[colName].values)\n        if (type == 'S'):\n            colValues = colStandardization(colName,df[colName].values)\n        df[colName] = colValues.tolist()\n    \n    return(df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2a56b3cb-8aef-4c3b-a24e-bc4bd776d194","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Read CSV\n\n# File location and type\nfile_location = \"/FileStore/tables/test/california_housing.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"707ba3ae-0ced-462c-babf-b25cbf22be6e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# convert dataframe to pandas spark dataframe\npsdf = ps.DataFrame(df)\nprint(psdf.head(5))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"63d98d8c-b8a8-46f1-92c6-7bb8ebb7831a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# info\nprint(psdf.info())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"97aa38f2-dd97-4014-a6ec-dba9033604a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# drop ocean proximity\n# del col\npsdf = psdf.drop('ocean_proximity', axis=1)\nprint(psdf.info())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9df6ac4a-f89c-451e-a1f4-02197c6bd1c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check nulls\nprint('\\n*** Columns With Nulls ***')\nprint(psdf.isnull().sum()) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a4482192-e1d9-4cd9-ac9e-02d88e974d2d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# handle nulls if required\nprint('\\n*** Handle Nulls ***')\npsdf = HandleNullsWithMean(psdf)\nprint(\"Done ...\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b1c7e5e6-7181-4d30-a665-83cf796b40b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check nulls\nprint('\\n*** Columns With Nulls ***')\nprint(psdf.isnull().sum()) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f6599e44-5dbc-4a7f-be5c-3cd8aa422c6f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Normailze Data**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"aa7fec03-175a-4659-9a47-c725aaa9a577","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# copy dataframe\npsdfn = psdf.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"35c6df79-d61a-403f-a6bb-6d117cc748f5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check mean\nprint('\\n*** Mean In Columns ***')\nprint(psdfn.mean())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"923ed948-aabd-4ad7-9540-f89b0f775364","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# handle normalization\nprint('\\n*** Normalize Data ***')\npsdfn = ScaleData(psdfn, 'N', ['ser','latitude','longitude','median_house_value'])\nprint('Done ...')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d591a445-c004-47d0-b86d-36f76e7131fe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check mean\nprint('\\n*** Mean In Columns ***')\nprint(psdfn.mean())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"00c4b683-1534-4819-a350-c81e28423925","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Standardize Data**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38422b9b-3ba4-4cc8-b951-99a8ea2c02c3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# copy dataframe\npsdfs = psdf.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"edaa77f8-fe55-4340-8ec8-d25dff9bc308","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check mean\nprint('\\n*** Mean In Columns ***')\nprint(psdfs.mean())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fca66f42-af23-4b03-a765-36f941bd6a46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# handle standardization\nprint('\\n*** Standardize Data ***')\npsdfs = ScaleData(psdfs, 'S', ['ser','latitude','longitude','median_house_value'])\nprint('Done ...')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f9037f14-0e47-433f-99a1-d7c6e2977c26","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check mean\nprint('\\n*** Mean In Columns ***')\nprint(psdfs.mean())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c3a3fafc-583f-4c4f-bbdc-96e5dc3d2eb3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"05D-DataScaling","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
