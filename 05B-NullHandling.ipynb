{"cells":[{"cell_type":"markdown","source":["####Null Handling"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"faf3556b-fb87-4a7b-8141-6136abb4dad5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# imports\n# hides all warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# import pandas for spark\nimport pyspark.pandas as ps\n# pandas \nimport pandas as pd\n# numpy\nimport numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"73d54e0d-f5bd-49f5-a896-aa9a3b234e1c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# data frame handle nulls replace with ReplVals of the columns as per replBy vars\n\"\"\"\ndesc:\n    HandleNulls - handle nulls from all cols in df except exclCols \n                nulls being replaced replBy value\nusage: \n    HandleNulls(df, replBy, colClass) \nparams:\n    df datarame, \n    replBy - mean, median, minimum (of mean & median), maximum (of mean & median) \n    exclCols - col to ignore while transformation \n\"\"\"\ndef HandleNulls(df, replBy, lExclCols=[]):\n    # orig col names\n    colNames = df.columns.to_list()\n    # if not list convert to list\n    if not isinstance(lExclCols, list):\n        lExclCols = [lExclCols]\n    #print(lExclCols)\n    # if not empty, create a dataframe of ExclCols\n    if lExclCols != []:\n        for vExclCol in lExclCols:\n            colNames.remove(vExclCol)\n    # handle outlier for each col\n    for colName in colNames:\n        if ((df[colName].isnull()).sum() > 0):\n            if (replBy == \"mean\"):\n                replVals = df[colName].mean()\n            elif (replBy == \"median\"):\n                replVals = df[colName].median()\n            elif (replBy == \"minimum\"):\n                replVals = min(df[colName].mean(),df[colName].median())\n            elif (replBy == \"maximum\"):\n                replVals = max(df[colName].mean(),df[colName].median())\n            # replace\n            df[colName] = df[colName].fillna(replVals)\n    return df\n\n# data frame handle nulls replace with mean of the columns \ndef HandleNullsWithMean(df, lExclCols=[]):\n    df = HandleNulls(df, \"mean\", lExclCols)\n    return df\n\n# data frame handle nulls replace with median of the columns \ndef HandleNullsWithMedian(df, lExclCols=[]):\n    df = HandleNulls(df, \"median\", lExclCols)\n    return df\n\n# data frame handle nulls replace with min(mean,median) of the columns \ndef HandleNullsWithMinOfMM(df, lExclCols=[]):\n    df = HandleNulls(df, \"minimum\", lExclCols)\n    return df\n\n# data frame handle nulls replace with max(mean,median) of the columns \ndef HandleNullsWithMaxOfMM(df, lExclCols=[]):\n    df = HandleNulls(df, \"maximum\", lExclCols)\n    return df\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"18d4c67f-a489-48f8-9e10-5384ac8fd6d9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# read csv\n\n# just change location & name as applicable\n# File location and type\nfile_location = \"/FileStore/tables/test/airquality.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"20b54fd3-44cb-4c7a-872d-d8fdfa77ec77","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# convert dataframe to pandas spark dataframe\npsdf = ps.DataFrame(df)\nprint(psdf.head(5))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ee4ada4d-ae6d-4281-8ca4-efd060f366ab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check nulls\nprint('\\n*** Columns With Nulls ***')\nprint(psdf.isnull().sum()) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c09bb3d8-2760-44a1-88b5-70096e10b666","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# handle nulls if required\nprint('\\n*** Handle Nulls ***')\ndf = HandleNullsWithMean(psdf)\nprint(\"Done ...\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3f8405a6-5ae0-4c1f-93b0-1c5f574a3c78","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check nulls\nprint('\\n*** Columns With Nulls ***')\nprint(psdf.isnull().sum()) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"77097d4d-dbd6-4268-965e-96334ea2fa4c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"05B-NullHandling","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
