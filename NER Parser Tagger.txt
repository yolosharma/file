TAGGER
for token in doc_1:
    print(token.text,'=>',token.tag_) # gives result in the form of tags
for token in doc_1:
    print(token.text,'=>>',token.tag)  # gives result in form of tag number

POS
for token in doc_1:
    print(token.text,'==>',token.pos_)
for token in doc1:
    print(token.text,'==>',token.dep_) # finds dependence between tokens

pos_count=doc_1.count_by(spacy.attrs.POS) # pos count
pos_count

VISULAISATION OF POS
from spacy import displacy
displacy.render(doc_1,style='dep')  # Dependence
options={'compact':'True','color':'blue'}

displacy.render(doc_1,style='dep',options=options)

CONVERTING A TEXT INTO DATAFRAME OF POS AND TOKENS
token=[]
for sent in nlp.pipe(text_df['Sentence']):
    if sent.has_annotation('DEP'):
        token.append([word.text for word in sent] )
token
token=[]
pos=[]
for sent in nlp.pipe(text_df['Sentence']):
    if sent.has_annotation('DEP'):
        token.append([word.text for word in sent] )
        pos.append([word.pos_ for word in sent])
print(token)
print(pos)
text_df['Token']=token
text_df['POS']=pos


for chunk in doc1.noun_chunks:
    print(chunk.text, '==>',chunk.label_) # noun chunks
for ent in doc2.ents:
    print(ent.text,'==>',ent.label_) # entities 

ent_list=[]
for ent in doc2.ents:
    ent_list.append(ent.label_) # list of entities

for ent in doc2.ents:
    print(ent.text, ent.label_) # List of tuples of text and the respective entities

NER FOR WEB DATA (# when we have big document how we retrieve the name present in that is done by NER)
import requests 
from bs4 import BeautifulSoup
url='https://en.wikipedia.org/wiki/India'
print(url)
request=requests.get(url)
print(request)
request=request.text
print(request)
soup_request=BeautifulSoup(request)
print(soup_request)
text= soup_request.body.text
print(text)
type(text)

CONVERTING STR TO DOC USING NLP
doc3=nlp(text)
type(doc3)
for token in doc3:
    print(token.text)
len(doc3)

from collections import Counter
Counter(ent_list)

ENTITIES MOST APPEARED
most_ent=[]
for ent in doc3.ents:
    most_ent.append(ent.text)
print(most_ent)
Counter(most_ent).most_common()
Counter(most_ent).most_common(10)  # top 10 occuring 


NO OF SENTENCES IN DOC
sent_count=0
for sent in doc.sents:
    sent_count=sent_count+1
    print(sent_count,'=>',sent)
print('Total no of sentences:',sent_count)
