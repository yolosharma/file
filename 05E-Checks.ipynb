{"cells":[{"cell_type":"markdown","source":["####Checks - SameValuesCols | UniqValuesCols | NullValuesCols"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1c9b01ee-6bb1-4576-b63a-9edbdd822be9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# imports\n# hides all warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# import pandas for spark\nimport pyspark.pandas as ps\n# pandas \nimport pandas as pd\n# numpy\nimport numpy as np\n# variance\nfrom pyspark.sql.functions import variance"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"be44ec84-6afb-455c-9091-1a385a33ec88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# identify columns where all value are same (uniqVals=1)\n# identify columns where two or less values are unique (uniqVals=2)\n# identify columns where three or less values are unique (uniqVals=3)\n# works only where all cols are numeric\ndef SameValuesCols(df, lExclCols=[], uniqVals=1, Verbose = False):\n    # orig col names\n    colNames = df.columns.to_list()\n    # if not list convert to list\n    if not isinstance(lExclCols, list):\n        lExclCols = [lExclCols]\n    #print(lExclCols)\n    # if not empty, create a dataframe of ExclCols\n    if lExclCols != []:\n        for vExclCol in lExclCols:\n            colNames.remove(vExclCol)\n    # handle same value for each col\n    lRetVals = []\n    dsRetValue = pd.Series() \n    for colName in colNames:\n        cntUniq = df[colName].nunique()\n        #cntRecs  = len(df.index)\n        dsRetValue[colName] = '%7d' % cntUniq\n        if (cntUniq <= uniqVals):\n            lRetVals.append(colName)\n    if (Verbose):       \n        print(dsRetValue)    \n    return lRetVals"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"65cee2de-e228-4b83-80f8-823b012107f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# identify columns with more than 100% unique values\ndef UniqValuesCols(df, lExclCols=[], Percent=0.95, Verbose = False):\n    # orig col names\n    colNames = df.columns.to_list()\n    # if not list convert to list\n    if not isinstance(lExclCols, list):\n        lExclCols = [lExclCols]\n    #print(lExclCols)\n    # if not empty, create a dataframe of ExclCols\n    if lExclCols != []:\n        for vExclCol in lExclCols:\n            colNames.remove(vExclCol)\n    # handle uniq values for each col\n    dsRetValue = pd.Series() \n    lRetVals = []\n    for colName in colNames:\n        cntUniq = df[colName].nunique()\n        cntRecs  = len(df.index)\n        perRecs  = cntUniq / cntRecs\n        dsRetValue[colName] = '%.2f' % perRecs\n        if perRecs >= Percent:\n            lRetVals.append(colName)\n    if (Verbose):       \n        print(dsRetValue)    \n    return lRetVals\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"be4138ae-9249-4b6d-92a0-832c4f296905","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# identify columns with more than 50% null values\ndef NullValuesCols(df, lExclCols=[], Percent=0.50, Verbose = False):\n    # currently can check only 100% same values so Percent has to 1 (100%)\n    if (Percent < 0) & (Percent>1) :\n        Percent=0.5\n    # orig col names\n    colNames = df.columns.to_list()\n    #print(colNames)\n    #print(type(colNames))\n    # if not list convert to list\n    if not isinstance(lExclCols, list):\n        lExclCols = [lExclCols]\n    #print(lExclCols)\n    # if not empty, create a dataframe of ExclCols\n    if lExclCols != []:\n        for vExclCol in lExclCols:\n            colNames.remove(vExclCol)\n    # handle null values for each col\n    dsRetValue = pd.Series() \n    lRetVals = []\n    for colName in colNames:\n        cntNulls = df[colName].isnull().sum()\n        cntRecs  = len(df.index)\n        perRecs  = cntNulls / cntRecs\n        #print(colName)\n        #print(perRecs)\n        #print(Percent)\n        if perRecs >= Percent:\n            lRetVals.append(colName)\n        dsRetValue[colName] = '%.2f' % perRecs\n    if (Verbose):       \n        print(dsRetValue)    \n    return (lRetVals)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"33e94cda-368a-4837-979b-682a819e95dd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Read CSV\n\n# File location and type\nfile_location = \"/FileStore/tables/test/california_housing.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"707ba3ae-0ced-462c-babf-b25cbf22be6e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# convert dataframe to pandas spark dataframe\npsdf = ps.DataFrame(df)\nprint(psdf.head(5))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"63d98d8c-b8a8-46f1-92c6-7bb8ebb7831a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# info\nprint(psdf.info())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"97aa38f2-dd97-4014-a6ec-dba9033604a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# drop ocean proximity\n# del col\npsdf = psdf.drop('ocean_proximity', axis=1)\nprint(psdf.info())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9df6ac4a-f89c-451e-a1f4-02197c6bd1c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# drop col if all the values (100%) are same\nprint(\"\\n*** Same Value Cols Drop ***\")\nlDropCols = SameValuesCols(psdf, \"median_house_value\", 1, True)\nprint(lDropCols)\n#if lDropCols != []:\n#    df = df.drop(lDropCols, axis=1)\nprint(\"Done ...\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"af561ec0-64f4-452e-ba61-5a8065a16627","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n# drop col if contains 100% unique values\nprint(\"\\n*** Uniq Value Cols Drop ***\")\nlDropCols = UniqValuesCols(psdf, \"median_house_value\", 1)\nprint(lDropCols)\n#if lDropCols != []:\n#    df = df.drop(lDropCols, axis=1)\nprint(\"Done ...\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a4542594-84a6-4a84-86e4-9e06dfec59a0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# drop col if more than 50% null values\nprint(\"\\n*** Null Value Cols Drop ***\")\nlDropCols = NullValuesCols(psdf, \"median_house_value\", 0.50)\nprint(lDropCols)\n#if lDropCols != []:\n#    psdf = psdf.drop(lDropCols, axis=1)\nprint(\"Done ...\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7cf13d33-840e-4761-99d9-e8dff3ce9c5e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"05E-Checks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
